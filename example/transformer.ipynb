{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T16:16:44.670309Z",
     "start_time": "2018-12-10T16:16:44.665401Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Tuple, Dict, Callable, Optional, Any, Sequence, Mapping, NamedTuple\n",
    "from attrdict import AttrDict\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T16:16:45.544052Z",
     "start_time": "2018-12-10T16:16:44.869997Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T16:16:45.554241Z",
     "start_time": "2018-12-10T16:16:45.545665Z"
    }
   },
   "outputs": [],
   "source": [
    "from model.transformer import Transformer\n",
    "from datasource.sample_ds import SampleDataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T16:16:45.666019Z",
     "start_time": "2018-12-10T16:16:45.555079Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T16:16:45.728494Z",
     "start_time": "2018-12-10T16:16:45.668373Z"
    }
   },
   "outputs": [],
   "source": [
    "hparams = AttrDict()\n",
    "hparams.num_layers = 4\n",
    "hparams.num_units = 512\n",
    "hparams.num_filter_units = hparams.num_units * 4\n",
    "hparams.num_heads = 8\n",
    "hparams.dropout_rate = 0.1\n",
    "hparams.max_length = 50\n",
    "hparams.batch_size = 32\n",
    "hparams.learning_rate = 0.001\n",
    "hparams.warmup_steps = 4000\n",
    "hparams.num_epochs = 50\n",
    "hparams.vocab_size = 3278\n",
    "hparams.data_path = './data/'\n",
    "hparams.ckpt_path = './ckpt/vanilla/l{}_u{}/model.ckpt'.format(hparams.num_layers, hparams.num_units)\n",
    "hparams.log_dir = './logs/vanilla/l{}_u{}'.format(hparams.num_layers, hparams.num_units)\n",
    "hparams1 = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T16:16:45.847783Z",
     "start_time": "2018-12-10T16:16:45.785232Z"
    }
   },
   "outputs": [],
   "source": [
    "hparams2 = AttrDict()\n",
    "hparams2.num_layers = 6\n",
    "hparams2.num_units = 512\n",
    "hparams2.num_filter_units = hparams2.num_units * 4\n",
    "hparams2.num_heads = 8\n",
    "hparams2.dropout_rate = 0.1\n",
    "hparams2.max_length = 50\n",
    "hparams2.batch_size = 64\n",
    "hparams2.learning_rate = 0.001\n",
    "hparams2.warmup_steps = 4000\n",
    "hparams2.num_epochs = 30\n",
    "hparams2.vocab_size = 3278\n",
    "hparams2.data_path = './data/'\n",
    "hparams2.ckpt_path = './ckpt/vanilla/l{}_u{}/model.ckpt'.format(hparams2.num_layers, hparams2.num_units)\n",
    "hparams2.log_dir = './logs/vanilla/l{}_u{}'.format(hparams2.num_layers, hparams2.num_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T16:16:56.338070Z",
     "start_time": "2018-12-10T16:16:56.333582Z"
    }
   },
   "outputs": [],
   "source": [
    "# eager\n",
    "def worker(hparams, gpu_id):\n",
    "    with tf.device('/gpu:{}'.format(gpu_id)):\n",
    "        ds = SampleDataSource(hparams)\n",
    "        model = Transformer(hparams, True)\n",
    "        optimizer = tf.train.AdamOptimizer(model.learning_rate, beta1=0.9, beta2=0.98, epsilon=1e-09)\n",
    "        model.load(optimizer)\n",
    "        writer = tf.contrib.summary.create_file_writer(hparams['log_dir'])\n",
    "        writer.set_as_default()\n",
    "        model.fit(ds, optimizer, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:35:28.345721Z",
     "start_time": "2018-12-10T07:35:15.187002Z"
    }
   },
   "outputs": [],
   "source": [
    "# graph mode\n",
    "def worker_graph(hparams, gpu_id):\n",
    "    gpu_id = 1\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:{}'.format(gpu_id)):\n",
    "            ds = SampleDataSource(hparams)\n",
    "            model = Transformer(hparams, True)\n",
    "            model.build_graph()\n",
    "            learning_rate = model.learning_rate()\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.98, epsilon=1e-09)\n",
    "            tf_config = tf.ConfigProto(\n",
    "                allow_soft_placement=True,\n",
    "                gpu_options=tf.GPUOptions(\n",
    "                    allow_growth=True\n",
    "                )\n",
    "            )\n",
    "            with tf.Session(config=tf_config) as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                for e in range(hparams['num_epochs']):\n",
    "                    ds.shuffle()\n",
    "                    batch = ds.feed_dict(model, hparams['batch_size'], True)\n",
    "                    start = time.time()\n",
    "                    for b in batch:\n",
    "                        inputs, targets = b[0], b[2]\n",
    "                        loss_op = model.loss_op\n",
    "                        grads = tf.gradients(loss_op, tf.trainable_variables())\n",
    "                        train_op = optimizer.apply_gradients(zip(grads, tf.trainable_variables()), model.global_step)\n",
    "\n",
    "                        _, loss, acc = sess.run([train_op, model.loss_op, model.acc_op], feed_dict={\n",
    "                            model.encoder_inputs_ph: inputs,\n",
    "                            model.decoder_inputs_ph: targets,\n",
    "                            model.is_training_ph: True\n",
    "                        })\n",
    "                        step = sess.run(model.global_step)\n",
    "                        with tf.contrib.summary.record_summaries_every_n_global_steps(10):\n",
    "                            tf.contrib.summary.scalar('summary/acc', acc)\n",
    "                            tf.contrib.summary.scalar('summary/loss', loss)\n",
    "                            tf.contrib.summary.scalar('summary/learning_rate', model.learning_rate())\n",
    "                    print('elapsed: ', time.time() - start)\n",
    "                    model.save(optimizer)\n",
    "                    print('{} epoch finished. now {} step, loss: {:.4f}, acc: {:.4f}'.format(e, step, loss ,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T16:17:00.566627Z",
     "start_time": "2018-12-10T16:17:00.563898Z"
    }
   },
   "outputs": [],
   "source": [
    "process_0 = Process(target=worker,args=(hparams2, 1))\n",
    "#process_1 = Process(target=worker,args=(hparams2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T16:17:02.009125Z",
     "start_time": "2018-12-10T16:17:02.003518Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kentaro.nakanishi/.pyenv/versions/3.6.5/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kentaro.nakanishi/.pyenv/versions/3.6.5/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-1a20639367fe>\", line 8, in worker\n",
      "    writer = tf.contrib.summary.create_file_writer(hparams['log_dir'])\n",
      "  File \"/home/kentaro.nakanishi/.local/share/virtualenvs/universal_transformer-eUF550pf/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/home/kentaro.nakanishi/.local/share/virtualenvs/universal_transformer-eUF550pf/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\n",
      "    module = importlib.import_module(self.__name__)\n",
      "  File \"/home/kentaro.nakanishi/.local/share/virtualenvs/universal_transformer-eUF550pf/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/kentaro.nakanishi/.local/share/virtualenvs/universal_transformer-eUF550pf/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 49, in <module>\n",
      "    from tensorflow.contrib import distributions\n",
      "  File \"/home/kentaro.nakanishi/.local/share/virtualenvs/universal_transformer-eUF550pf/lib/python3.6/site-packages/tensorflow/contrib/distributions/__init__.py\", line 38, in <module>\n",
      "    from tensorflow.contrib.distributions.python.ops.estimator import *\n",
      "  File \"/home/kentaro.nakanishi/.local/share/virtualenvs/universal_transformer-eUF550pf/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/estimator.py\", line 21, in <module>\n",
      "    from tensorflow.contrib.learn.python.learn.estimators.head import _compute_weighted_loss\n",
      "  File \"/home/kentaro.nakanishi/.local/share/virtualenvs/universal_transformer-eUF550pf/lib/python3.6/site-packages/tensorflow/contrib/learn/__init__.py\", line 96, in <module>\n",
      "    from tensorflow.contrib.learn.python.learn import *\n",
      "  File \"/home/kentaro.nakanishi/.local/share/virtualenvs/universal_transformer-eUF550pf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/__init__.py\", line 28, in <module>\n",
      "    from tensorflow.contrib.learn.python.learn import *\n",
      "  File \"/home/kentaro.nakanishi/.local/share/virtualenvs/universal_transformer-eUF550pf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/__init__.py\", line 28, in <module>\n",
      "    from tensorflow.contrib.learn.python.learn import basic_session_run_hooks\n",
      "  File \"/home/kentaro.nakanishi/.local/share/virtualenvs/universal_transformer-eUF550pf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/basic_session_run_hooks.py\", line 53, in <module>\n",
      "    basic_session_run_hooks.NanTensorHook)\n",
      "  File \"/home/kentaro.nakanishi/.local/share/virtualenvs/universal_transformer-eUF550pf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 173, in deprecated_alias\n",
      "    class _NewClass(func_or_class):  # pylint: disable=missing-docstring\n",
      "  File \"/home/kentaro.nakanishi/.local/share/virtualenvs/universal_transformer-eUF550pf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 180, in _NewClass\n",
      "    __module__ = _call_location(outer=True)\n",
      "  File \"/home/kentaro.nakanishi/.local/share/virtualenvs/universal_transformer-eUF550pf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 78, in _call_location\n",
      "    frame = tf_inspect.currentframe()\n",
      "  File \"/home/kentaro.nakanishi/.local/share/virtualenvs/universal_transformer-eUF550pf/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py\", line 42, in currentframe\n",
      "    return _inspect.stack()[1][0]\n",
      "  File \"/home/kentaro.nakanishi/.pyenv/versions/3.6.5/lib/python3.6/inspect.py\", line 1494, in stack\n",
      "    return getouterframes(sys._getframe(1), context)\n",
      "  File \"/home/kentaro.nakanishi/.pyenv/versions/3.6.5/lib/python3.6/inspect.py\", line 1471, in getouterframes\n",
      "    frameinfo = (frame,) + getframeinfo(frame, context)\n",
      "  File \"/home/kentaro.nakanishi/.pyenv/versions/3.6.5/lib/python3.6/inspect.py\", line 1445, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/home/kentaro.nakanishi/.pyenv/versions/3.6.5/lib/python3.6/inspect.py\", line 768, in findsource\n",
      "    file = getsourcefile(object)\n",
      "  File \"/home/kentaro.nakanishi/.pyenv/versions/3.6.5/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/kentaro.nakanishi/.pyenv/versions/3.6.5/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "process_0.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T16:17:02.126422Z",
     "start_time": "2018-12-10T16:17:02.093335Z"
    }
   },
   "outputs": [],
   "source": [
    "process_1.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.3",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

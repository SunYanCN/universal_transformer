{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:48:58.180919Z",
     "start_time": "2018-12-10T07:48:58.174499Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Tuple, Dict, Callable, Optional, Any, Sequence, Mapping, NamedTuple\n",
    "from attrdict import AttrDict\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:48:59.169267Z",
     "start_time": "2018-12-10T07:48:58.422404Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:48:59.179162Z",
     "start_time": "2018-12-10T07:48:59.171120Z"
    }
   },
   "outputs": [],
   "source": [
    "from model.attention import SelfAttention, MultiheadAttention\n",
    "from model.embedding import EmbeddingSharedWeights\n",
    "from model.ffn import FeedForwardNetwork\n",
    "from model.layer_utils import LayerWrapper, LayerNormalization\n",
    "from model import model_utils\n",
    "from datasource.sample_ds import SampleDataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:48:59.258155Z",
     "start_time": "2018-12-10T07:48:59.180347Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:48:59.409992Z",
     "start_time": "2018-12-10T07:48:59.389831Z"
    }
   },
   "outputs": [],
   "source": [
    "hparams = AttrDict()\n",
    "#hparams.num_layers = 4\n",
    "hparams.num_units = 1024\n",
    "hparams.num_filter_units = hparams.num_units * 4\n",
    "hparams.num_heads = 8\n",
    "hparams.dropout_rate = 0.1\n",
    "hparams.max_length = 50\n",
    "hparams.batch_size = 64\n",
    "hparams.learning_rate = 0.001\n",
    "hparams.warmup_steps = 4000\n",
    "hparams.num_epochs = 2\n",
    "hparams.vocab_size = 3278\n",
    "hparams.data_path = './data/'\n",
    "hparams.ckpt_path = './ckpt/aut/u{}_2/model.ckpt'.format(hparams.num_units)\n",
    "hparams.log_dir = './logs/aut/u{}_2'.format(hparams.num_units)\n",
    "hparams.act_max_step = 20\n",
    "hparams.act_epsilon = 0.01\n",
    "hparams.act_loss_weight = 0.01\n",
    "hparams1 = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:00.187359Z",
     "start_time": "2018-12-10T07:49:00.181788Z"
    }
   },
   "outputs": [],
   "source": [
    "hparams2 = AttrDict()\n",
    "hparams2.num_layers = 6\n",
    "hparams2.num_units = 1024\n",
    "hparams2.num_filter_units = hparams2.num_units * 4\n",
    "hparams2.num_heads = 8\n",
    "hparams2.dropout_rate = 0.1\n",
    "hparams2.max_length = 50\n",
    "hparams2.batch_size = 64\n",
    "hparams2.learning_rate = 0.001\n",
    "hparams2.warmup_steps = 4000\n",
    "hparams2.num_epochs = 30\n",
    "hparams2.vocab_size = 3278\n",
    "hparams2.data_path = './data/'\n",
    "hparams2.ckpt_path = './ckpt/vanilla/l{}_u{}/model.ckpt'.format(hparams2.num_layers, hparams2.num_units)\n",
    "hparams2.log_dir = './logs/vanilla/l{}_u{}'.format(hparams2.num_layers, hparams2.num_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:00.187359Z",
     "start_time": "2018-12-10T07:49:00.181788Z"
    }
   },
   "outputs": [],
   "source": [
    "hparams3 = AttrDict()\n",
    "hparams3.num_layers = 1\n",
    "hparams3.num_units = 1024\n",
    "hparams3.num_filter_units = hparams3.num_units * 4\n",
    "hparams3.num_heads = 8\n",
    "hparams3.dropout_rate = 0.1\n",
    "hparams3.max_length = 50\n",
    "hparams3.batch_size = 64\n",
    "hparams3.learning_rate = 0.001\n",
    "hparams3.warmup_steps = 4000\n",
    "hparams3.num_epochs = 20\n",
    "hparams3.vocab_size = 3278\n",
    "hparams3.data_path = './data/'\n",
    "hparams3.ckpt_path = './ckpt/vanilla/l{}_u{}/model.ckpt'.format(hparams3.num_layers, hparams3.num_units)\n",
    "hparams3.log_dir = './logs/vanilla/l{}_u{}'.format(hparams3.num_layers, hparams3.num_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:01.340249Z",
     "start_time": "2018-12-10T07:49:01.024823Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = SampleDataSource(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:01.373090Z",
     "start_time": "2018-12-10T07:49:01.341717Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class UniversalTransformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, hparams, is_train):\n",
    "        super(UniversalTransformer, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.is_train = is_train\n",
    "        self.embedding_layer = EmbeddingSharedWeights(hparams['vocab_size'], hparams['num_units'])\n",
    "        self.encoder_stack = EncoderStack(hparams, is_train)\n",
    "        self.encoder_embedding_dropout = tf.keras.layers.Dropout(hparams['dropout_rate'])\n",
    "        \n",
    "        self.decoder_stack = DecoderStack(hparams, is_train)\n",
    "        self.decoder_embedding_dropout = tf.keras.layers.Dropout(hparams['dropout_rate'])\n",
    "        \n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "    \n",
    "    def call(self, inputs, targets: Optional[np.ndarray] = None):\n",
    "        attention_bias = model_utils.get_padding_bias(inputs)\n",
    "        encoder_outputs, enc_ponders, enc_remainders = self._encode(inputs, attention_bias)\n",
    "        logits, dec_ponders, dec_remainders = self._decode(encoder_outputs, targets, attention_bias)\n",
    "\n",
    "        if targets is None:\n",
    "            raise Exception()\n",
    "        enc_act_loss = tf.reduce_mean(enc_ponders + enc_remainders)\n",
    "        dec_act_loss = tf.reduce_mean(dec_ponders + dec_remainders)\n",
    "        act_loss = self.hparams['act_loss_weight'] * (enc_act_loss + dec_act_loss)\n",
    "        if self.is_train:\n",
    "            with tf.contrib.summary.record_summaries_every_n_global_steps(10):\n",
    "                tf.contrib.summary.scalar('summary/ponder_times_enc', tf.reduce_mean(enc_ponders))\n",
    "                tf.contrib.summary.scalar('summary/ponder_times_dec', tf.reduce_mean(dec_ponders))\n",
    "            \n",
    "        return logits, act_loss\n",
    "        \n",
    "    def build_graph(self):\n",
    "        with tf.name_scope('graph'):\n",
    "            self.is_training_ph = tf.placeholder(name='is_training', shape=(), dtype=bool)\n",
    "            self.encoder_inputs_ph = tf.placeholder(name='encoder_inputs', shape=[self.hparams['batch_size'], self.hparams['max_length']], dtype=tf.int32)\n",
    "            self.decoder_inputs_ph = tf.placeholder(name='decoder_inputs', shape=[self.hparams['batch_size'], self.hparams['max_length']], dtype=tf.int32)\n",
    "\n",
    "            self.logits = self.call(self.encoder_inputs_ph, self.decoder_inputs_ph)\n",
    "            \n",
    "            self.loss_op = self.loss(self.encoder_inputs_ph, self.decoder_inputs_ph)\n",
    "            self.acc_op = self.acc(self.encoder_inputs_ph, self.decoder_inputs_ph)\n",
    "        \n",
    "    def save(self, optimizer):\n",
    "        checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                         model=self,\n",
    "                                         optimizer_step=self.global_step)\n",
    "        checkpoint.save(self.hparams['ckpt_path'])\n",
    "        \n",
    "    def load(self, optimizer):\n",
    "        ckpt_path = tf.train.latest_checkpoint(os.path.dirname(self.hparams['ckpt_path']))\n",
    "        if ckpt_path:\n",
    "            checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                                     model=self,\n",
    "                                                     optimizer_step=self.global_step)\n",
    "            checkpoint.restore(ckpt_path)\n",
    "            print('restored')\n",
    "        else:\n",
    "            print('not restored because no checkpoint found')\n",
    "    \n",
    "    def loss(self, inputs, targets):\n",
    "        pad = tf.to_float(tf.not_equal(targets, 0))\n",
    "        onehot_targets = tf.one_hot(targets, self.hparams['vocab_size'])\n",
    "        logits, act_loss = self(inputs, targets)\n",
    "        cross_ents = tf.losses.softmax_cross_entropy(\n",
    "            onehot_labels=onehot_targets,\n",
    "            logits=logits\n",
    "        )\n",
    "        loss = tf.reduce_sum(cross_ents * pad) / tf.reduce_sum(pad)\n",
    "        with tf.contrib.summary.record_summaries_every_n_global_steps(10):\n",
    "            tf.contrib.summary.scalar('summary/target_loss', loss)\n",
    "            tf.contrib.summary.scalar('summary/act_loss', act_loss)\n",
    "        return loss + act_loss\n",
    "    \n",
    "    def acc(self, inputs, targets):\n",
    "        logits, _ = self(inputs, targets)\n",
    "        predicted_ids = tf.to_int32(tf.argmax(logits, axis=2))\n",
    "        correct = tf.equal(predicted_ids, targets)\n",
    "        pad = tf.to_float(tf.not_equal(targets, 0))\n",
    "        acc = tf.reduce_sum(tf.to_float(correct) * pad) / (tf.reduce_sum(pad))\n",
    "        return acc\n",
    "        \n",
    "    def grads(self, inputs, targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss(inputs, targets)\n",
    "        return tape.gradient(loss, self.variables)\n",
    "    \n",
    "    def fit(self, ds, optimizer, writer):\n",
    "        \"\"\" Function to train the model, using the selected optimizer and\n",
    "            for the desired number of epochs. It also stores the accuracy\n",
    "            of the model after each epoch.\n",
    "        \"\"\"\n",
    "        for e in range(self.hparams['num_epochs']):\n",
    "            batch = ds.feed_dict(None, self.hparams['batch_size'], True)\n",
    "            start = time.time()\n",
    "            for b in batch:\n",
    "                inputs, targets = b[0], b[2]\n",
    "                loss = self.loss(inputs, targets)\n",
    "                acc = self.acc(inputs, targets)\n",
    "                \n",
    "                grads = self.grads(inputs, targets)\n",
    "                optimizer.apply_gradients(zip(grads, self.variables), self.global_step)\n",
    "                step = self.global_step.numpy()\n",
    "                with tf.contrib.summary.record_summaries_every_n_global_steps(10):\n",
    "                    tf.contrib.summary.scalar('summary/acc', acc)\n",
    "                    tf.contrib.summary.scalar('summary/loss', loss)\n",
    "                    tf.contrib.summary.scalar('summary/learning_rate', self.learning_rate())\n",
    "            print('elapsed: ', time.time() - start)\n",
    "            self.save(optimizer)\n",
    "            print('{} epoch finished. now {} step, loss: {:.4f}, acc: {:.4f}'.format(e, step, loss ,acc))\n",
    "        \n",
    "    def predict(self, encoder_outputs, bias):\n",
    "        pass\n",
    "        \n",
    "    def _encode(self, inputs, attention_bias):\n",
    "        embedded_inputs = self.embedding_layer(inputs)\n",
    "        inputs_padding = model_utils.get_padding(inputs)\n",
    "\n",
    "        if self.is_train:\n",
    "            encoder_inputs = self.encoder_embedding_dropout(embedded_inputs)\n",
    "        return self.encoder_stack(encoder_inputs, attention_bias, inputs_padding)\n",
    "    \n",
    "    def _decode(self, encoder_outputs, targets, attention_bias):\n",
    "        decoder_inputs = self.embedding_layer(targets)\n",
    "        decoder_inputs = tf.pad(decoder_inputs, [[0, 0], [1, 0], [0, 0]])[:, :-1, :]\n",
    "        # add positional encoding\n",
    "        length = tf.shape(decoder_inputs)[1]\n",
    "        decoder_inputs += model_utils.get_position_encoding(length, self.hparams['num_units'])\n",
    "        \n",
    "        if self.is_train:\n",
    "            decoder_inputs = self.decoder_embedding_dropout(decoder_inputs)\n",
    "\n",
    "        decoder_self_attention_bias = model_utils.get_decoder_self_attention_bias(length)\n",
    "        outputs, dec_ponders, dec_remainders = self.decoder_stack(decoder_inputs, encoder_outputs, decoder_self_attention_bias, attention_bias)\n",
    "        logits = self.embedding_layer.linear(outputs)\n",
    "        return logits, dec_ponders, dec_remainders\n",
    "    \n",
    "    def learning_rate(self):\n",
    "        step = tf.to_float(self.global_step)\n",
    "        rate = tf.minimum(step ** -0.5, step * self.hparams['warmup_steps'] ** -1.5) * self.hparams['num_units'] ** -0.5\n",
    "        return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ACT(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, batch_size, length, hidden_size):\n",
    "        super(ACT, self).__init__()\n",
    "        \n",
    "        self.halting_probability = tf.zeros((batch_size, length), name='halting_probability')\n",
    "        self.remainders = tf.zeros((batch_size, length), name=\"remainder\")\n",
    "        self.n_updates = tf.zeros((batch_size, length), name=\"n_updates\")\n",
    "        \n",
    "    def call(self, pondering, halt_threshold):\n",
    "        # 今現在まだ計算している symbol だけ取ってくるマスク\n",
    "        still_running = tf.cast(tf.less(self.halting_probability, 1.0), tf.float32)\n",
    "\n",
    "        # 今回の stepondering で停止する symbol のマスク、halt_threshold を超えているかどうかチェックしている\n",
    "        new_halted = tf.greater(self.halting_probability + pondering * still_running, halt_threshold)\n",
    "        new_halted = tf.cast(new_halted, tf.float32) * still_running\n",
    "\n",
    "        # ここまででも今回のでも停止しないもののマスク\n",
    "        still_running_now = tf.less_equal(self.halting_probability + pondering * still_running, halt_threshold)\n",
    "        still_running_now = tf.cast(still_running_now, tf.float32) * still_running\n",
    "\n",
    "        # まだ停止していない symbol について、停止する確率を更新\n",
    "        self.halting_probability += pondering * still_running\n",
    "\n",
    "        # 今回停止した symbol について、remainder の計算して停止確率を更新\n",
    "        self.remainders += new_halted * (1 - self.halting_probability)\n",
    "        self.halting_probability += new_halted * self.remainders\n",
    "\n",
    "        # 今回更新があった symbol について更新回数を足す\n",
    "        self.n_updates += still_running + new_halted\n",
    "\n",
    "        # 新しい state をどれだけ output に入れるかの weights を計算し、shape をあわせる\n",
    "        # ここで既に停止している symbol の係数は 0 になるため、値は変わらない\n",
    "        update_weights = pondering * still_running + new_halted * self.remainders\n",
    "        update_weights = tf.expand_dims(update_weights, -1)\n",
    "        \n",
    "        return update_weights\n",
    "    \n",
    "    def should_continue(self, threshold) -> bool:\n",
    "        return tf.reduce_any(tf.less(self.halting_probability, threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:06.150889Z",
     "start_time": "2018-12-10T07:49:06.146038Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderStack(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, hparams, is_train):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        \n",
    "        self_attention_layer = SelfAttention(hparams['num_units'], hparams['num_heads'], hparams['dropout_rate'], is_train)\n",
    "        ffn_layer = FeedForwardNetwork(hparams['num_units'], hparams['num_filter_units'], hparams['dropout_rate'], is_train)\n",
    "        self.self_attention_wrapper = LayerWrapper(self_attention_layer, hparams['num_units'], hparams['dropout_rate'], is_train)\n",
    "        self.ffn_wrapper = LayerWrapper(ffn_layer, hparams['num_units'], hparams['dropout_rate'], is_train)\n",
    "        self.output_norm = LayerNormalization(hparams['num_units'])\n",
    "        self.pondering_layer = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, use_bias=True, bias_initializer=tf.constant_initializer(1.0))\n",
    "        \n",
    "    \n",
    "    def call(self, encoder_inputs, attention_bias, inputs_padding):\n",
    "        batch_size, length, hidden_size = tf.unstack(tf.shape(encoder_inputs))\n",
    "        act = ACT(batch_size, length, hidden_size)\n",
    "        halt_threshold = 1.0 - hparams['act_epsilon']\n",
    "        \n",
    "        state = encoder_inputs\n",
    "        previous_state = tf.zeros_like(state, name='previous_state')\n",
    "        for step in range(self.hparams['act_max_step']):\n",
    "            # 終了条件を確認\n",
    "            if not act.should_continue(halt_threshold):\n",
    "                break\n",
    "            \n",
    "            # position & timestep encoding\n",
    "            state += model_utils.get_position_encoding(self.hparams['max_length'], hidden_size)\n",
    "            state += model_utils.get_timestep_encoding(step, self.hparams['act_max_step'], hidden_size)\n",
    "            \n",
    "            # pondering 判断のための特徴を計算\n",
    "            pondering = self.pondering_layer(state)\n",
    "            pondering = tf.squeeze(pondering, axis=-1)\n",
    "            \n",
    "            # proceed act step\n",
    "            update_weights = act(pondering, halt_threshold)\n",
    "            \n",
    "            state = self.self_attention_wrapper(state, attention_bias)\n",
    "            state = self.ffn_wrapper(state, inputs_padding)\n",
    "            \n",
    "            # ここまでの state と weighted sum を取り、 previous_state を更新\n",
    "            new_state = (state * update_weights) + (previous_state * (1 - update_weights))\n",
    "            previous_state = new_state\n",
    "\n",
    "        return self.output_norm(new_state), act.n_updates, act.remainders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:06.277463Z",
     "start_time": "2018-12-10T07:49:06.271825Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderStack(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, hparams, is_train):\n",
    "        super(DecoderStack, self).__init__()\n",
    "        self.my_layers = []\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        self_attention_layer = SelfAttention(hparams['num_units'], hparams['num_heads'], hparams['dropout_rate'], is_train)\n",
    "        enc_dec_attention_layer = MultiheadAttention(hparams['num_units'], hparams['num_heads'], hparams['dropout_rate'], is_train)\n",
    "        ffn_layer = FeedForwardNetwork(hparams['num_units'], hparams['num_filter_units'], hparams['dropout_rate'], is_train)\n",
    "        self.self_attention_wrapper = LayerWrapper(self_attention_layer, hparams['num_units'], hparams['dropout_rate'], is_train)\n",
    "        self.enc_dec_attention_wrapper = LayerWrapper(enc_dec_attention_layer, hparams['num_units'], hparams['dropout_rate'], is_train)\n",
    "        self.ffn_wrapper = LayerWrapper(ffn_layer, hparams['num_units'], hparams['dropout_rate'], is_train)\n",
    "        self.output_norm = LayerNormalization(hparams['num_units'])\n",
    "        self.pondering_layer = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, use_bias=True, bias_initializer=tf.constant_initializer(1.0))\n",
    "    \n",
    "    def call(self, decoder_inputs, encoder_outputs, decoder_self_attention_bias, attention_bias):\n",
    "        batch_size, length, hidden_size = tf.unstack(tf.shape(decoder_inputs))\n",
    "        act = ACT(batch_size, length, hidden_size)\n",
    "        halt_threshold = 1.0 - hparams['act_epsilon']\n",
    "        \n",
    "        state = decoder_inputs\n",
    "        previous_state = tf.zeros_like(state, name='previous_state')\n",
    "        for step in range(self.hparams['act_max_step']):\n",
    "            # position and timestep encoding\n",
    "            state += model_utils.get_position_encoding(self.hparams['max_length'], hidden_size)\n",
    "            state += model_utils.get_timestep_encoding(step, self.hparams['act_max_step'], hidden_size)\n",
    "            \n",
    "            # pondering 判断のための特徴を計算\n",
    "            pondering = self.pondering_layer(state)\n",
    "            pondering = tf.squeeze(pondering, axis=-1)\n",
    "            \n",
    "            # proceed act step\n",
    "            update_weights = act(pondering, halt_threshold)\n",
    "            \n",
    "            state = self.self_attention_wrapper(state, decoder_self_attention_bias)\n",
    "            state = self.enc_dec_attention_wrapper(state, encoder_outputs, attention_bias)\n",
    "            state = self.ffn_wrapper(state)\n",
    "            \n",
    "            # ここまでの state と weighted sum を取り、 previous_state を更新\n",
    "            new_state = (state * update_weights) + (previous_state * (1 - update_weights))\n",
    "            previous_state = new_state\n",
    "            \n",
    "        return self.output_norm(new_state), act.n_updates, act.remainders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:08.661762Z",
     "start_time": "2018-12-10T07:49:08.658686Z"
    }
   },
   "outputs": [],
   "source": [
    "# eager\n",
    "def worker(hparams, gpu_id):\n",
    "    with tf.device('/gpu:{}'.format(gpu_id)):\n",
    "        ds = SampleDataSource(hparams)\n",
    "        model = UniversalTransformer(hparams, True)\n",
    "        optimizer = tf.train.AdamOptimizer(model.learning_rate, beta1=0.9, beta2=0.98, epsilon=1e-09)\n",
    "        model.load(optimizer)\n",
    "        writer = tf.contrib.summary.create_file_writer(hparams['log_dir'])\n",
    "        writer.set_as_default()\n",
    "        model.fit(ds, optimizer, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:35:28.345721Z",
     "start_time": "2018-12-10T07:35:15.187002Z"
    }
   },
   "outputs": [],
   "source": [
    "# graph mode\n",
    "def worker_graph(hparams, gpu_id):\n",
    "    gpu_id = 1\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:{}'.format(gpu_id)):\n",
    "            ds = SampleDataSource(hparams)\n",
    "            model = Transformer(hparams, True)\n",
    "            model.build_graph()\n",
    "            learning_rate = model.learning_rate()\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.98, epsilon=1e-09)\n",
    "            tf_config = tf.ConfigProto(\n",
    "                allow_soft_placement=True,\n",
    "                gpu_options=tf.GPUOptions(\n",
    "                    allow_growth=True\n",
    "                )\n",
    "            )\n",
    "            with tf.Session(config=tf_config) as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                for e in range(hparams['num_epochs']):\n",
    "                    ds.shuffle()\n",
    "                    batch = ds.feed_dict(model, hparams['batch_size'], True)\n",
    "                    start = time.time()\n",
    "                    for b in batch:\n",
    "                        inputs, targets = b[0], b[2]\n",
    "                        loss_op = model.loss_op\n",
    "                        grads = tf.gradients(loss_op, tf.trainable_variables())\n",
    "                        train_op = optimizer.apply_gradients(zip(grads, tf.trainable_variables()), model.global_step)\n",
    "\n",
    "                        _, loss, acc = sess.run([train_op, model.loss_op, model.acc_op], feed_dict={\n",
    "                            model.encoder_inputs_ph: inputs,\n",
    "                            model.decoder_inputs_ph: targets,\n",
    "                            model.is_training_ph: True\n",
    "                        })\n",
    "                        step = sess.run(model.global_step)\n",
    "                        with tf.contrib.summary.record_summaries_every_n_global_steps(10):\n",
    "                            tf.contrib.summary.scalar('summary/acc', acc)\n",
    "                            tf.contrib.summary.scalar('summary/loss', loss)\n",
    "                            tf.contrib.summary.scalar('summary/learning_rate', model.learning_rate())\n",
    "                    print('elapsed: ', time.time() - start)\n",
    "                    model.save(optimizer)\n",
    "                    print('{} epoch finished. now {} step, loss: {:.4f}, acc: {:.4f}'.format(e, step, loss ,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:13.305414Z",
     "start_time": "2018-12-10T07:49:13.301262Z"
    }
   },
   "outputs": [],
   "source": [
    "process_0 = Process(target=worker,args=(hparams1, 1))\n",
    "process_1 = Process(target=worker,args=(hparams1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:25.953298Z",
     "start_time": "2018-12-10T07:49:25.939746Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_0.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:26.036229Z",
     "start_time": "2018-12-10T07:49:26.023338Z"
    }
   },
   "outputs": [],
   "source": [
    "process_1.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker(hparams1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.3",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

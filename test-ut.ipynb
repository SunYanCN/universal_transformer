{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:48:58.180919Z",
     "start_time": "2018-12-10T07:48:58.174499Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Tuple, Dict, Callable, Optional, Any, Sequence, Mapping, NamedTuple\n",
    "from attrdict import AttrDict\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:48:59.169267Z",
     "start_time": "2018-12-10T07:48:58.422404Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:48:59.179162Z",
     "start_time": "2018-12-10T07:48:59.171120Z"
    }
   },
   "outputs": [],
   "source": [
    "from model.attention import SelfAttention, MultiheadAttention\n",
    "from model.embedding import EmbeddingSharedWeights\n",
    "from model.ffn import FeedForwardNetwork\n",
    "from model.layer_utils import LayerWrapper, LayerNormalization\n",
    "from model import model_utils\n",
    "from datasource.sample_ds import SampleDataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:48:59.258155Z",
     "start_time": "2018-12-10T07:48:59.180347Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:48:59.409992Z",
     "start_time": "2018-12-10T07:48:59.389831Z"
    }
   },
   "outputs": [],
   "source": [
    "hparams = AttrDict()\n",
    "hparams.num_layers = 4\n",
    "hparams.num_units = 2048\n",
    "hparams.num_filter_units = hparams.num_units * 4\n",
    "hparams.num_heads = 8\n",
    "hparams.dropout_rate = 0.1\n",
    "hparams.max_length = 50\n",
    "hparams.batch_size = 64\n",
    "hparams.learning_rate = 0.001\n",
    "hparams.warmup_steps = 4000\n",
    "hparams.num_epochs = 30\n",
    "hparams.vocab_size = 3278\n",
    "hparams.data_path = './data/'\n",
    "hparams.ckpt_path = './ckpt/ut/l{}_u{}/model.ckpt'.format(hparams.num_layers, hparams.num_units)\n",
    "hparams.log_dir = './logs/ut/l{}_u{}'.format(hparams.num_layers, hparams.num_units)\n",
    "hparams1 = hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:00.187359Z",
     "start_time": "2018-12-10T07:49:00.181788Z"
    }
   },
   "outputs": [],
   "source": [
    "hparams2 = AttrDict()\n",
    "hparams2.num_layers = 4\n",
    "hparams2.num_units = 1024\n",
    "hparams2.num_filter_units = hparams2.num_units * 4\n",
    "hparams2.num_heads = 8\n",
    "hparams2.dropout_rate = 0.1\n",
    "hparams2.max_length = 50\n",
    "hparams2.batch_size = 64\n",
    "hparams2.learning_rate = 0.001\n",
    "hparams2.warmup_steps = 4000\n",
    "hparams2.num_epochs = 30\n",
    "hparams2.vocab_size = 3278\n",
    "hparams2.data_path = './data/'\n",
    "hparams2.ckpt_path = './ckpt/ut/l{}_u{}/model.ckpt'.format(hparams2.num_layers, hparams2.num_units)\n",
    "hparams2.log_dir = './logs/ut/l{}_u{}'.format(hparams2.num_layers, hparams2.num_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:00.187359Z",
     "start_time": "2018-12-10T07:49:00.181788Z"
    }
   },
   "outputs": [],
   "source": [
    "hparams3 = AttrDict()\n",
    "hparams3.num_layers = 1\n",
    "hparams3.num_units = 1024\n",
    "hparams3.num_filter_units = hparams3.num_units * 4\n",
    "hparams3.num_heads = 8\n",
    "hparams3.dropout_rate = 0.1\n",
    "hparams3.max_length = 50\n",
    "hparams3.batch_size = 64\n",
    "hparams3.learning_rate = 0.001\n",
    "hparams3.warmup_steps = 4000\n",
    "hparams3.num_epochs = 20\n",
    "hparams3.vocab_size = 3278\n",
    "hparams3.data_path = './data/'\n",
    "hparams3.ckpt_path = './ckpt/ut/l{}_u{}/model.ckpt'.format(hparams3.num_layers, hparams3.num_units)\n",
    "hparams3.log_dir = './logs/ut/l{}_u{}'.format(hparams3.num_layers, hparams3.num_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:01.340249Z",
     "start_time": "2018-12-10T07:49:01.024823Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = SampleDataSource(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:01.373090Z",
     "start_time": "2018-12-10T07:49:01.341717Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class UniversalTransformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, hparams, is_train):\n",
    "        super(UniversalTransformer, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.is_train = is_train\n",
    "        self.embedding_layer = EmbeddingSharedWeights(hparams['vocab_size'], hparams['num_units'])\n",
    "        self.encoder_stack = EncoderStack(hparams, is_train)\n",
    "        self.encoder_embedding_dropout = tf.keras.layers.Dropout(hparams['dropout_rate'])\n",
    "        \n",
    "        self.decoder_stack = DecoderStack(hparams, is_train)\n",
    "        self.decoder_embedding_dropout = tf.keras.layers.Dropout(hparams['dropout_rate'])\n",
    "        \n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "    \n",
    "    def call(self, inputs, targets: Optional[np.ndarray] = None):\n",
    "        attention_bias = model_utils.get_padding_bias(inputs)\n",
    "        encoder_outputs = self._encode(inputs, attention_bias)\n",
    "        \n",
    "        if targets is None:\n",
    "            logits = self._decode(encoder_outputs, targets, attention_bias)\n",
    "            #raise Exception()\n",
    "            return logits #self.predict(encoder_outputs, attention_bias)\n",
    "        else:\n",
    "            logits = self._decode(encoder_outputs, targets, attention_bias)\n",
    "            return logits\n",
    "        \n",
    "    def build_graph(self):\n",
    "        with tf.name_scope('graph'):\n",
    "            self.is_training_ph = tf.placeholder(name='is_training', shape=(), dtype=bool)\n",
    "            self.encoder_inputs_ph = tf.placeholder(name='encoder_inputs', shape=[self.hparams['batch_size'], self.hparams['max_length']], dtype=tf.int32)\n",
    "            self.decoder_inputs_ph = tf.placeholder(name='decoder_inputs', shape=[self.hparams['batch_size'], self.hparams['max_length']], dtype=tf.int32)\n",
    "\n",
    "            self.logits = self.call(self.encoder_inputs_ph, self.decoder_inputs_ph)\n",
    "            \n",
    "            self.loss_op = self.loss(self.encoder_inputs_ph, self.decoder_inputs_ph)\n",
    "            self.acc_op = self.acc(self.encoder_inputs_ph, self.decoder_inputs_ph)\n",
    "        \n",
    "    def save(self, optimizer):\n",
    "        checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                                 model=self,\n",
    "                                                 optimizer_step=self.global_step)\n",
    "        checkpoint.save(self.hparams['ckpt_path'])\n",
    "        \n",
    "    def load(self, optimizer):\n",
    "        ckpt_path = tf.train.latest_checkpoint(os.path.dirname(self.hparams['ckpt_path']))\n",
    "        if ckpt_path:\n",
    "            checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                                     model=self,\n",
    "                                                     optimizer_step=self.global_step)\n",
    "            checkpoint.restore(ckpt_path)\n",
    "            print('restored')\n",
    "        else:\n",
    "            print('not restored because no checkpoint found')\n",
    "    \n",
    "    def loss(self, inputs, targets):\n",
    "        pad = tf.to_float(tf.not_equal(targets, 0))\n",
    "        onehot_targets = tf.one_hot(targets, self.hparams['vocab_size'])\n",
    "        logits = self(inputs, targets)\n",
    "        cross_ents = tf.losses.softmax_cross_entropy(\n",
    "            onehot_labels=onehot_targets,\n",
    "            logits=logits\n",
    "        )\n",
    "        loss = tf.reduce_sum(cross_ents * pad) / tf.reduce_sum(pad)\n",
    "        return loss\n",
    "    \n",
    "    def acc(self, inputs, targets):\n",
    "        logits = self(inputs, targets)\n",
    "        predicted_ids = tf.to_int32(tf.argmax(logits, axis=2))\n",
    "        correct = tf.equal(predicted_ids, targets)\n",
    "        pad = tf.to_float(tf.not_equal(targets, 0))\n",
    "        acc = tf.reduce_sum(tf.to_float(correct) * pad) / (tf.reduce_sum(pad))\n",
    "        return acc\n",
    "        \n",
    "    def grads(self, inputs, targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss(inputs, targets)\n",
    "        return tape.gradient(loss, self.variables)\n",
    "    \n",
    "    def fit(self, ds, optimizer, writer):\n",
    "        \"\"\" Function to train the model, using the selected optimizer and\n",
    "            for the desired number of epochs. It also stores the accuracy\n",
    "            of the model after each epoch.\n",
    "        \"\"\"\n",
    "        for e in range(self.hparams['num_epochs']):\n",
    "            batch = ds.feed_dict(None, self.hparams['batch_size'], True)\n",
    "            start = time.time()\n",
    "            for b in batch:\n",
    "                inputs, targets = b[0], b[2]\n",
    "                loss = self.loss(inputs, targets)\n",
    "                acc = self.acc(inputs, targets)\n",
    "                \n",
    "                grads = self.grads(inputs, targets)\n",
    "                optimizer.apply_gradients(zip(grads, self.variables), self.global_step)\n",
    "                step = self.global_step.numpy()\n",
    "                with tf.contrib.summary.record_summaries_every_n_global_steps(10):\n",
    "                    tf.contrib.summary.scalar('summary/acc', acc)\n",
    "                    tf.contrib.summary.scalar('summary/loss', loss)\n",
    "                    tf.contrib.summary.scalar('summary/learning_rate', self.learning_rate())\n",
    "            print('elapsed: ', time.time() - start)\n",
    "            self.save(optimizer)\n",
    "            print('{} epoch finished. now {} step, loss: {:.4f}, acc: {:.4f}'.format(e, step, loss ,acc))\n",
    "        \n",
    "    def predict(self, encoder_outputs, bias):\n",
    "        pass\n",
    "        \n",
    "    def _encode(self, inputs, attention_bias):\n",
    "        embedded_inputs = self.embedding_layer(inputs)\n",
    "        inputs_padding = model_utils.get_padding(inputs)\n",
    "\n",
    "        if self.is_train:\n",
    "            encoder_inputs = self.encoder_embedding_dropout(embedded_inputs)\n",
    "        return self.encoder_stack(encoder_inputs, attention_bias, inputs_padding)\n",
    "    \n",
    "    def _decode(self, encoder_outputs, targets, attention_bias):\n",
    "        decoder_inputs = self.embedding_layer(targets)\n",
    "        decoder_inputs = tf.pad(decoder_inputs, [[0, 0], [1, 0], [0, 0]])[:, :-1, :]\n",
    "        # add positional encoding\n",
    "        length = tf.shape(decoder_inputs)[1]\n",
    "        decoder_inputs += model_utils.get_position_encoding(length, self.hparams['num_units'])\n",
    "        \n",
    "        if self.is_train:\n",
    "            decoder_inputs = self.decoder_embedding_dropout(decoder_inputs)\n",
    "\n",
    "        decoder_self_attention_bias = model_utils.get_decoder_self_attention_bias(length)\n",
    "        outputs = self.decoder_stack(decoder_inputs, encoder_outputs, decoder_self_attention_bias, attention_bias)\n",
    "        logits = self.embedding_layer.linear(outputs)\n",
    "        return logits\n",
    "    \n",
    "    def learning_rate(self):\n",
    "        step = tf.to_float(self.global_step)\n",
    "        rate = tf.minimum(step ** -0.5, step * self.hparams['warmup_steps'] ** -1.5) * self.hparams['num_units'] ** -0.5\n",
    "        return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:06.150889Z",
     "start_time": "2018-12-10T07:49:06.146038Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderStack(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, hparams, is_train):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.num_layers = hparams['num_layers']\n",
    "        \n",
    "        self_attention_layer = SelfAttention(hparams['num_units'], hparams['num_heads'], hparams['dropout_rate'], is_train)\n",
    "        ffn_layer = FeedForwardNetwork(hparams['num_units'], hparams['num_filter_units'], hparams['dropout_rate'], is_train)\n",
    "        self.self_attention_wrapper = LayerWrapper(self_attention_layer, hparams['num_units'], hparams['dropout_rate'], is_train)\n",
    "        self.ffn_wrapper = LayerWrapper(ffn_layer, hparams['num_units'], hparams['dropout_rate'], is_train)\n",
    "            \n",
    "        self.output_norm = LayerNormalization(hparams['num_units'])\n",
    "    \n",
    "    def call(self, encoder_inputs, attention_bias, inputs_padding):\n",
    "        for l in range(self.num_layers):\n",
    "            encoder_inputs += model_utils.get_position_encoding(self.hparams['max_length'], self.hparams['num_units'])\n",
    "            encoder_inputs += model_utils.get_timestep_encoding(l, self.num_layers, self.hparams['num_units'])\n",
    "            encoder_inputs = self.self_attention_wrapper(encoder_inputs, attention_bias)\n",
    "            encoder_inputs = self.ffn_wrapper(encoder_inputs, inputs_padding)\n",
    "            \n",
    "        return self.output_norm(encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:06.277463Z",
     "start_time": "2018-12-10T07:49:06.271825Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderStack(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, hparams, is_train):\n",
    "        super(DecoderStack, self).__init__()\n",
    "        self.my_layers = []\n",
    "        self.hparams = hparams\n",
    "        self.num_layers = hparams['num_layers']\n",
    "        self_attention_layer = SelfAttention(hparams['num_units'], hparams['num_heads'], hparams['dropout_rate'], is_train)\n",
    "        enc_dec_attention_layer = MultiheadAttention(hparams['num_units'], hparams['num_heads'], hparams['dropout_rate'], is_train)\n",
    "        ffn_layer = FeedForwardNetwork(hparams['num_units'], hparams['num_filter_units'], hparams['dropout_rate'], is_train)\n",
    "        self.self_attention_wrapper = LayerWrapper(self_attention_layer, hparams['num_units'], hparams['dropout_rate'], is_train)\n",
    "        self.enc_dec_attention_wrapper = LayerWrapper(enc_dec_attention_layer, hparams['num_units'], hparams['dropout_rate'], is_train)\n",
    "        self.ffn_wrapper = LayerWrapper(ffn_layer, hparams['num_units'], hparams['dropout_rate'], is_train)\n",
    "            \n",
    "        self.output_norm = LayerNormalization(hparams['num_units'])\n",
    "    \n",
    "    def call(self, decoder_inputs, encoder_outputs, decoder_self_attention_bias, attention_bias):\n",
    "        for l in range(self.num_layers):\n",
    "            decoder_inputs += model_utils.get_position_encoding(self.hparams['max_length'], self.hparams['num_units'])\n",
    "            decoder_inputs += model_utils.get_timestep_encoding(l, self.num_layers, self.hparams['num_units'])\n",
    "            decoder_inputs = self.self_attention_wrapper(decoder_inputs, decoder_self_attention_bias)\n",
    "            decoder_inputs = self.enc_dec_attention_wrapper(decoder_inputs, encoder_outputs, attention_bias)\n",
    "            decoder_inputs = self.ffn_wrapper(decoder_inputs)\n",
    "            \n",
    "        return self.output_norm(decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:15:10.214425Z",
     "start_time": "2018-12-10T07:15:08.825961Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 実験用\n",
    "#with tf.device('cpu:0'):\n",
    "model = UniversalTransformer(hparams, True)\n",
    "#parallel_model = multi_gpu_model(model, gpus=2)\n",
    "optimizer = tf.train.AdamOptimizer(model.learning_rate, beta1=0.9, beta2=0.98, epsilon=1e-09)\n",
    "model.load(optimizer)\n",
    "writer = tf.contrib.summary.create_file_writer(hparams['log_dir'])\n",
    "writer.set_as_default()\n",
    "model.fit(ds, optimizer, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:15:10.215812Z",
     "start_time": "2018-12-10T07:15:27.703Z"
    }
   },
   "outputs": [],
   "source": [
    "a = tf.keras.layers.Input(shape=(hparams['batch_size'], hparams['max_length']))\n",
    "b = Transformer()()\n",
    "kerasmodel = tf.keras.Model(inputs=a, outputs=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:08.661762Z",
     "start_time": "2018-12-10T07:49:08.658686Z"
    }
   },
   "outputs": [],
   "source": [
    "# eager\n",
    "def worker(hparams, gpu_id):\n",
    "    with tf.device('/gpu:{}'.format(gpu_id)):\n",
    "        ds = SampleDataSource(hparams)\n",
    "        model = UniversalTransformer(hparams, True)\n",
    "        optimizer = tf.train.AdamOptimizer(model.learning_rate, beta1=0.9, beta2=0.98, epsilon=1e-09)\n",
    "        model.load(optimizer)\n",
    "        writer = tf.contrib.summary.create_file_writer(hparams['log_dir'])\n",
    "        writer.set_as_default()\n",
    "        model.fit(ds, optimizer, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:35:28.345721Z",
     "start_time": "2018-12-10T07:35:15.187002Z"
    }
   },
   "outputs": [],
   "source": [
    "# graph mode\n",
    "def worker_graph(hparams, gpu_id):\n",
    "    gpu_id = 1\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:{}'.format(gpu_id)):\n",
    "            ds = SampleDataSource(hparams)\n",
    "            model = Transformer(hparams, True)\n",
    "            model.build_graph()\n",
    "            learning_rate = model.learning_rate()\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.98, epsilon=1e-09)\n",
    "            tf_config = tf.ConfigProto(\n",
    "                allow_soft_placement=True,\n",
    "                gpu_options=tf.GPUOptions(\n",
    "                    allow_growth=True\n",
    "                )\n",
    "            )\n",
    "            with tf.Session(config=tf_config) as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                for e in range(hparams['num_epochs']):\n",
    "                    ds.shuffle()\n",
    "                    batch = ds.feed_dict(model, hparams['batch_size'], True)\n",
    "                    start = time.time()\n",
    "                    for b in batch:\n",
    "                        inputs, targets = b[0], b[2]\n",
    "                        loss_op = model.loss_op\n",
    "                        grads = tf.gradients(loss_op, tf.trainable_variables())\n",
    "                        train_op = optimizer.apply_gradients(zip(grads, tf.trainable_variables()), model.global_step)\n",
    "\n",
    "                        _, loss, acc = sess.run([train_op, model.loss_op, model.acc_op], feed_dict={\n",
    "                            model.encoder_inputs_ph: inputs,\n",
    "                            model.decoder_inputs_ph: targets,\n",
    "                            model.is_training_ph: True\n",
    "                        })\n",
    "                        step = sess.run(model.global_step)\n",
    "                        with tf.contrib.summary.record_summaries_every_n_global_steps(10):\n",
    "                            tf.contrib.summary.scalar('summary/acc', acc)\n",
    "                            tf.contrib.summary.scalar('summary/loss', loss)\n",
    "                            tf.contrib.summary.scalar('summary/learning_rate', model.learning_rate())\n",
    "                    print('elapsed: ', time.time() - start)\n",
    "                    model.save(optimizer)\n",
    "                    print('{} epoch finished. now {} step, loss: {:.4f}, acc: {:.4f}'.format(e, step, loss ,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:13.305414Z",
     "start_time": "2018-12-10T07:49:13.301262Z"
    }
   },
   "outputs": [],
   "source": [
    "process_0 = Process(target=worker,args=(hparams1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:25.953298Z",
     "start_time": "2018-12-10T07:49:25.939746Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not restored because no checkpoint found\n",
      "elapsed:  5889.694883346558\n",
      "0 epoch finished. now 4855 step, loss: 1.0067, acc: 0.4154\n",
      "elapsed:  5882.262910366058\n",
      "1 epoch finished. now 9710 step, loss: 0.9432, acc: 0.4472\n",
      "elapsed:  5887.116944074631\n",
      "2 epoch finished. now 14565 step, loss: 0.9216, acc: 0.4564\n",
      "elapsed:  5885.355655431747\n",
      "3 epoch finished. now 19420 step, loss: 0.8963, acc: 0.4648\n",
      "elapsed:  5879.579460144043\n",
      "4 epoch finished. now 24275 step, loss: 0.8758, acc: 0.4799\n",
      "elapsed:  5884.973979949951\n",
      "5 epoch finished. now 29130 step, loss: 0.8466, acc: 0.4933\n",
      "elapsed:  5888.301313400269\n",
      "6 epoch finished. now 33985 step, loss: 0.8341, acc: 0.4933\n",
      "elapsed:  5879.296197891235\n",
      "7 epoch finished. now 38840 step, loss: 0.8198, acc: 0.4841\n",
      "elapsed:  5882.53919005394\n",
      "8 epoch finished. now 43695 step, loss: 0.7925, acc: 0.5168\n",
      "elapsed:  5883.542256355286\n",
      "9 epoch finished. now 48550 step, loss: 0.7721, acc: 0.5168\n",
      "elapsed:  5884.873591661453\n",
      "10 epoch finished. now 53405 step, loss: 0.7705, acc: 0.5201\n",
      "elapsed:  5886.626988172531\n",
      "11 epoch finished. now 58260 step, loss: 0.7430, acc: 0.5343\n",
      "elapsed:  5884.4252552986145\n",
      "12 epoch finished. now 63115 step, loss: 0.7298, acc: 0.5318\n",
      "elapsed:  5887.494434833527\n",
      "13 epoch finished. now 67970 step, loss: 0.7118, acc: 0.5419\n"
     ]
    }
   ],
   "source": [
    "process_0.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T07:49:26.036229Z",
     "start_time": "2018-12-10T07:49:26.023338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not restored because no checkpoint found\n",
      "elapsed:  2868.419594526291\n",
      "0 epoch finished. now 4855 step, loss: 1.0016, acc: 0.4204\n",
      "elapsed:  2819.362753868103\n",
      "1 epoch finished. now 9710 step, loss: 0.9412, acc: 0.4539\n",
      "elapsed:  2801.6315672397614\n",
      "2 epoch finished. now 14565 step, loss: 0.9111, acc: 0.4648\n",
      "elapsed:  2819.3562150001526\n",
      "3 epoch finished. now 19420 step, loss: 0.8840, acc: 0.4749\n",
      "elapsed:  2813.922508955002\n",
      "4 epoch finished. now 24275 step, loss: 0.8680, acc: 0.4883\n",
      "elapsed:  2817.0905635356903\n",
      "5 epoch finished. now 29130 step, loss: 0.8537, acc: 0.4791\n",
      "elapsed:  2812.568452835083\n",
      "6 epoch finished. now 33985 step, loss: 0.8411, acc: 0.4933\n",
      "elapsed:  2815.533511161804\n",
      "7 epoch finished. now 38840 step, loss: 0.8297, acc: 0.4841\n",
      "elapsed:  2816.5636587142944\n",
      "8 epoch finished. now 43695 step, loss: 0.8215, acc: 0.5008\n",
      "elapsed:  2813.4811549186707\n",
      "9 epoch finished. now 48550 step, loss: 0.8070, acc: 0.5151\n",
      "elapsed:  2808.9723584651947\n",
      "10 epoch finished. now 53405 step, loss: 0.7940, acc: 0.5109\n",
      "elapsed:  2816.952795267105\n",
      "11 epoch finished. now 58260 step, loss: 0.7880, acc: 0.5159\n",
      "elapsed:  2810.045441389084\n",
      "12 epoch finished. now 63115 step, loss: 0.7782, acc: 0.5260\n",
      "elapsed:  2814.1532714366913\n",
      "13 epoch finished. now 67970 step, loss: 0.7743, acc: 0.5209\n",
      "elapsed:  2811.4162724018097\n",
      "14 epoch finished. now 72825 step, loss: 0.7696, acc: 0.5310\n",
      "elapsed:  2815.1963217258453\n",
      "15 epoch finished. now 77680 step, loss: 0.7570, acc: 0.5310\n",
      "elapsed:  2814.526947259903\n",
      "16 epoch finished. now 82535 step, loss: 0.7510, acc: 0.5427\n",
      "elapsed:  2821.887409210205\n",
      "17 epoch finished. now 87390 step, loss: 0.7397, acc: 0.5444\n",
      "elapsed:  2813.553840637207\n",
      "18 epoch finished. now 92245 step, loss: 0.7307, acc: 0.5452\n",
      "elapsed:  2815.5227789878845\n",
      "19 epoch finished. now 97100 step, loss: 0.7341, acc: 0.5452\n",
      "elapsed:  2824.3571004867554\n",
      "20 epoch finished. now 101955 step, loss: 0.7212, acc: 0.5494\n",
      "elapsed:  2807.8035805225372\n",
      "21 epoch finished. now 106810 step, loss: 0.7099, acc: 0.5402\n",
      "elapsed:  2810.9467573165894\n",
      "22 epoch finished. now 111665 step, loss: 0.7045, acc: 0.5637\n",
      "elapsed:  2810.613893032074\n",
      "23 epoch finished. now 116520 step, loss: 0.6962, acc: 0.5503\n",
      "elapsed:  2818.5109782218933\n",
      "24 epoch finished. now 121375 step, loss: 0.6784, acc: 0.5637\n",
      "elapsed:  2817.6861288547516\n",
      "25 epoch finished. now 126230 step, loss: 0.6805, acc: 0.5595\n",
      "elapsed:  2812.445047855377\n",
      "26 epoch finished. now 131085 step, loss: 0.6780, acc: 0.5670\n",
      "elapsed:  2814.4817912578583\n",
      "27 epoch finished. now 135940 step, loss: 0.6664, acc: 0.5812\n",
      "elapsed:  2815.26638507843\n",
      "28 epoch finished. now 140795 step, loss: 0.6629, acc: 0.5662\n",
      "elapsed:  2806.459244966507\n",
      "29 epoch finished. now 145650 step, loss: 0.6621, acc: 0.5787\n"
     ]
    }
   ],
   "source": [
    "process_1 = Process(target=worker,args=(hparams2, 2))\n",
    "process_1.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker(hparams1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.3",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
